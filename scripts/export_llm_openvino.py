#!/usr/bin/env python3
"""
LabGuardian â€” æœ¬åœ° LLM æ¨¡å‹è½¬æ¢è„šæœ¬

å°† HuggingFace æ¨¡å‹è½¬æ¢ä¸º OpenVINO INT4 æ ¼å¼, ç”¨äº NPU/GPU ç¦»çº¿æ¨ç†ã€‚

ä½¿ç”¨æ–¹æ³•:
  1. å®‰è£…ä¾èµ–:
     pip install optimum[openvino] nncf

  2. è¿è¡Œè½¬æ¢ (éœ€è”ç½‘, ä»…æ‰§è¡Œä¸€æ¬¡):
     python scripts/export_llm_openvino.py --model qwen2.5-1.5b
     python scripts/export_llm_openvino.py --model minicpm-1b
     python scripts/export_llm_openvino.py --model phi3-mini

  3. è½¬æ¢å®Œæˆå, å°† models/<model_name> ç›®å½•æ‹·è´åˆ° DK-2500 å³å¯ç¦»çº¿ä½¿ç”¨ã€‚

æ¨èæ¨¡å‹ (æŒ‰ä¼˜å…ˆçº§):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ æ¨¡å‹                      â”‚ å‚æ•°é‡    â”‚ ä¸­æ–‡èƒ½åŠ›  â”‚ INT4 å¤§å°   â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Qwen2.5-1.5B-Instruct   â”‚ 1.5B     â”‚ â˜…â˜…â˜…â˜…â˜…   â”‚ ~1.0 GB    â”‚
  â”‚ MiniCPM-1B-sft-bf16     â”‚ 1.2B     â”‚ â˜…â˜…â˜…â˜…    â”‚ ~0.7 GB    â”‚
  â”‚ Phi-3-mini-4k-instruct  â”‚ 3.8B     â”‚ â˜…â˜…â˜…     â”‚ ~2.2 GB    â”‚
  â”‚ Qwen2.5-0.5B-Instruct   â”‚ 0.5B     â”‚ â˜…â˜…â˜…     â”‚ ~0.4 GB    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æŠ€æœ¯å‚è€ƒ:
  - OpenVINO GenAI NPU Guide:
    docs.openvino.ai/2024/learn-openvino/llm_inference_guide/genai-guide-npu.html
  - NPU è¦æ±‚å¯¹ç§° INT4 é‡åŒ– (--sym --ratio 1.0)
"""

import argparse
import subprocess
import sys
from pathlib import Path

# æ¨¡å‹æ³¨å†Œè¡¨
MODELS = {
    "qwen2.5-1.5b": {
        "hf_id": "Qwen/Qwen2.5-1.5B-Instruct",
        "output_dir": "qwen2.5_1.5b_ov",
        "group_size": 128,
        "description": "ä¸­æ–‡æœ€ä½³ 1.5B çº§æ¨¡å‹ (é¦–é€‰)",
    },
    "qwen2.5-0.5b": {
        "hf_id": "Qwen/Qwen2.5-0.5B-Instruct",
        "output_dir": "qwen2.5_0.5b_ov",
        "group_size": 128,
        "description": "æå°æ¨¡å‹, é€‚åˆå†…å­˜ç´§å¼ çš„åœºæ™¯",
    },
    "minicpm-1b": {
        "hf_id": "openbmb/MiniCPM-1B-sft-bf16",
        "output_dir": "minicpm_1b_ov",
        "group_size": 128,
        "description": "æ¸…å/é¢å£ç«¯ä¾§æ¨¡å‹, ä¸­æ–‡ä¼˜ç§€",
    },
    "phi3-mini": {
        "hf_id": "microsoft/Phi-3-mini-4k-instruct",
        "output_dir": "phi3_mini_ov",
        "group_size": 128,
        "description": "å¾®è½¯ Phi-3, æ¨ç†èƒ½åŠ›å¼º",
    },
}

PROJECT_ROOT = Path(__file__).resolve().parent.parent
MODELS_DIR = PROJECT_ROOT / "models"


def export_model(model_key: str, device_target: str = "NPU"):
    """æ‰§è¡Œæ¨¡å‹è½¬æ¢"""
    if model_key not in MODELS:
        print(f"âŒ æœªçŸ¥æ¨¡å‹: {model_key}")
        print(f"   å¯é€‰: {', '.join(MODELS.keys())}")
        return False

    info = MODELS[model_key]
    output_path = MODELS_DIR / info["output_dir"]

    if output_path.exists() and any(output_path.glob("*.xml")):
        print(f"âš ï¸ æ¨¡å‹å·²å­˜åœ¨: {output_path}")
        print("   å¦‚éœ€é‡æ–°è½¬æ¢, è¯·å…ˆåˆ é™¤è¯¥ç›®å½•")
        return True

    print(f"{'='*60}")
    print(f"ğŸ“¦ è½¬æ¢æ¨¡å‹: {info['hf_id']}")
    print(f"   {info['description']}")
    print(f"   è¾“å‡ºç›®å½•: {output_path}")
    print(f"   é‡åŒ–: INT4 å¯¹ç§°, group_size={info['group_size']}")
    print(f"   ç›®æ ‡è®¾å¤‡: {device_target}")
    print(f"{'='*60}")

    # NPU éœ€è¦å¯¹ç§° INT4 é‡åŒ–
    cmd = [
        sys.executable, "-m", "optimum.exporters.openvino",
        "--model", info["hf_id"],
        "--weight-format", "int4",
        "--sym",
        "--ratio", "1.0",
        "--group-size", str(info["group_size"]),
        "--trust-remote-code",
        str(output_path),
    ]

    # ç­‰æ•ˆçš„ CLI å‘½ä»¤ (ä¾›æ‰‹åŠ¨æ‰§è¡Œ):
    cli_cmd = (
        f"optimum-cli export openvino "
        f"--model {info['hf_id']} "
        f"--weight-format int4 --sym --ratio 1.0 "
        f"--group-size {info['group_size']} "
        f"--trust-remote-code "
        f"{output_path}"
    )
    print(f"\nğŸ’¡ ç­‰æ•ˆ CLI å‘½ä»¤:\n   {cli_cmd}\n")

    try:
        subprocess.run(cmd, check=True)
        print(f"\nâœ… è½¬æ¢æˆåŠŸ: {output_path}")
        print(f"   æ¨¡å‹å¤§å°: {_dir_size_mb(output_path):.1f} MB")
        return True
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ è½¬æ¢å¤±è´¥: {e}")
        return False
    except FileNotFoundError:
        print("\nâŒ æœªæ‰¾åˆ° optimumã€‚è¯·å…ˆå®‰è£…:")
        print("   pip install optimum[openvino] nncf")
        return False


def verify_model(model_key: str):
    """éªŒè¯å·²è½¬æ¢çš„æ¨¡å‹å¯å¦åŠ è½½"""
    if model_key not in MODELS:
        return

    info = MODELS[model_key]
    output_path = MODELS_DIR / info["output_dir"]

    if not output_path.exists():
        print(f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {output_path}")
        return

    print(f"\nğŸ” éªŒè¯æ¨¡å‹: {output_path}")

    # å°è¯• openvino_genai
    try:
        import openvino_genai as ov_genai
        pipe = ov_genai.LLMPipeline(str(output_path), "CPU")  # CPU éªŒè¯å³å¯
        result = pipe.generate("ä½ å¥½", max_new_tokens=20, do_sample=False)
        print(f"   âœ… openvino_genai éªŒè¯é€šè¿‡")
        print(f"   å›å¤: {result[:100]}")
        return
    except ImportError:
        print("   âš ï¸ openvino_genai æœªå®‰è£…, å°è¯• optimum")
    except Exception as e:
        print(f"   âš ï¸ openvino_genai åŠ è½½å¤±è´¥: {e}")

    # å›é€€ optimum
    try:
        from optimum.intel.openvino import OVModelForCausalLM
        from transformers import AutoTokenizer

        model = OVModelForCausalLM.from_pretrained(str(output_path), device="CPU")
        tokenizer = AutoTokenizer.from_pretrained(str(output_path))
        inputs = tokenizer("ä½ å¥½", return_tensors="pt")
        output = model.generate(**inputs, max_new_tokens=20)
        result = tokenizer.decode(output[0], skip_special_tokens=True)
        print(f"   âœ… optimum-intel éªŒè¯é€šè¿‡")
        print(f"   å›å¤: {result[:100]}")
    except Exception as e:
        print(f"   âŒ éªŒè¯å¤±è´¥: {e}")


def _dir_size_mb(path: Path) -> float:
    """è®¡ç®—ç›®å½•å¤§å° (MB)"""
    total = sum(f.stat().st_size for f in path.rglob("*") if f.is_file())
    return total / (1024 * 1024)


def main():
    parser = argparse.ArgumentParser(
        description="LabGuardian LLM æ¨¡å‹è½¬æ¢å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python scripts/export_llm_openvino.py --model qwen2.5-1.5b
  python scripts/export_llm_openvino.py --model qwen2.5-1.5b --verify
  python scripts/export_llm_openvino.py --list
        """,
    )
    parser.add_argument(
        "--model", type=str, choices=list(MODELS.keys()),
        help="è¦è½¬æ¢çš„æ¨¡å‹"
    )
    parser.add_argument(
        "--verify", action="store_true",
        help="è½¬æ¢åéªŒè¯æ¨¡å‹"
    )
    parser.add_argument(
        "--list", action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"
    )
    parser.add_argument(
        "--device", type=str, default="NPU",
        choices=["CPU", "GPU", "NPU"],
        help="ç›®æ ‡æ¨ç†è®¾å¤‡"
    )

    args = parser.parse_args()

    if args.list:
        print("\nğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
        print(f"{'â”€'*60}")
        for key, info in MODELS.items():
            output_path = MODELS_DIR / info["output_dir"]
            status = "âœ… å·²è½¬æ¢" if output_path.exists() else "â¬œ æœªè½¬æ¢"
            print(f"  {key:20s} {status}  {info['description']}")
        print(f"{'â”€'*60}")
        print(f"\næ¨¡å‹å­˜å‚¨ç›®å½•: {MODELS_DIR}")
        return

    if not args.model:
        parser.print_help()
        return

    MODELS_DIR.mkdir(parents=True, exist_ok=True)

    success = export_model(args.model, args.device)

    if success and args.verify:
        verify_model(args.model)


if __name__ == "__main__":
    main()
