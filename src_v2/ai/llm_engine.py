"""
LLM æ¨ç†å¼•æ“æ¨¡å— (v2)
èŒè´£ï¼šæä¾›ç»Ÿä¸€çš„ AI é—®ç­”æ¥å£, è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åç«¯

åç«¯ä¼˜å…ˆçº§ (ä¸‰çº§é™çº§ç­–ç•¥):
  1. Cloud  â€” DeepSeek / Qwen ç­‰ OpenAI å…¼å®¹äº‘ç«¯ API (æœ€å¼º, éœ€è”ç½‘)
  2. Local  â€” Qwen2.5-1.5B / MiniCPM ç­‰å°æ¨¡å‹, é€šè¿‡ openvino_genai åœ¨ NPU/GPU ä¸Šè¿è¡Œ
  3. Rule   â€” åŸºäºç”µè·¯é¢†åŸŸè§„åˆ™çš„æ¨¡æ¿å¼•æ“ (é›¶ä¾èµ–, ç¦»çº¿å…œåº•)

æ¨¡å‹é€‰å‹å»ºè®® (Intel Core Ultra 5 225U Â· DK-2500):
  - é¦–é€‰: Qwen2.5-1.5B-Instruct (INT4) â€” ä¸­æ–‡æœ€å¥½çš„ 1B çº§æ¨¡å‹
  - æ¬¡é€‰: MiniCPM-1B-sft (INT4) â€” æ¸…å/é¢å£, ä¸“ä¸ºç«¯ä¾§è®¾è®¡
  - å¤‡é€‰: Phi-3-mini-4k-instruct (INT4) â€” æ¨ç†èƒ½åŠ›å¼º, ä¸­æ–‡ç•¥å¼±
  - å¼ƒç”¨: TinyLlama-1.1B â€” ä¸­æ–‡å‡ ä¹ä¸å¯ç”¨

è½¬æ¢å‘½ä»¤ (åœ¨æœ‰ç½‘ç¯å¢ƒæ‰§è¡Œä¸€æ¬¡):
  optimum-cli export openvino \\
    --model Qwen/Qwen2.5-1.5B-Instruct \\
    --weight-format int4 --sym --ratio 1.0 --group-size 128 \\
    --trust-remote-code \\
    models/qwen2.5_1.5b_ov

å‚è€ƒ:
  - OpenVINO GenAI NPU Guide: docs.openvino.ai/2024/learn-openvino/llm_inference_guide/genai-guide-npu.html
  - openvino_genai LLMPipeline: github.com/openvinotoolkit/openvino.genai
"""

import logging
import re
from abc import ABC, abstractmethod
from typing import Optional

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from config import llm as llm_cfg

logger = logging.getLogger(__name__)


class LLMBackend(ABC):
    """LLM åç«¯æŠ½è±¡åŸºç±»"""

    @abstractmethod
    def generate(self, system_prompt: str, user_message: str) -> str:
        """ç”Ÿæˆå›å¤"""
        ...

    @abstractmethod
    def is_ready(self) -> bool:
        """æ˜¯å¦å·²åŠ è½½å°±ç»ª"""
        ...


class CloudLLMBackend(LLMBackend):
    """äº‘ç«¯ API åç«¯ (DeepSeek / Moonshot / Qwen ç­‰ OpenAI å…¼å®¹æ¥å£)"""

    def __init__(self):
        self.client = None

    def load(self) -> bool:
        if not llm_cfg.is_cloud_ready:
            print("[LLM-Cloud] API Key æœªé…ç½®. è®¾ç½®ç¯å¢ƒå˜é‡ LG_API_KEY æˆ– DEEPSEEK_API_KEY")
            return False
        try:
            from openai import OpenAI
            self.client = OpenAI(
                api_key=llm_cfg.cloud_api_key,
                base_url=llm_cfg.cloud_base_url,
            )
            print(f"[LLM-Cloud] å·²è¿æ¥: {llm_cfg.cloud_model_name} @ {llm_cfg.cloud_base_url}")
            return True
        except Exception as e:
            print(f"[LLM-Cloud] è¿æ¥å¤±è´¥: {e}")
            return False

    def is_ready(self) -> bool:
        return self.client is not None

    def generate(self, system_prompt: str, user_message: str) -> str:
        if not self.is_ready():
            return "[Error] Cloud LLM not ready."
        try:
            response = self.client.chat.completions.create(
                model=llm_cfg.cloud_model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message},
                ],
                max_tokens=llm_cfg.max_tokens,
                temperature=llm_cfg.temperature,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"[Cloud Error] {e}"


class LocalLLMBackend(LLMBackend):
    """
    æœ¬åœ° LLM åç«¯ (OpenVINO GenAI)

    é’ˆå¯¹ DK-2500 (Intel Core Ultra 5 225U) ä¼˜åŒ–:
      - è®¾å¤‡é™çº§é“¾: NPU â†’ GPU (Arc iGPU) â†’ CPU
      - NPU ç¼–è¯‘ç¼“å­˜: é¦–æ¬¡ç¼–è¯‘åç¼“å­˜, äºŒæ¬¡å¯åŠ¨ç§’çº§
      - é¢„çƒ­æœºåˆ¶: åŠ è½½åå‘é€çŸ­ prompt é¢„ç¼–è¯‘è®¡ç®—å›¾, é¿å…é¦–æ¬¡å®é™…é—®ç­”å¡é¡¿

    åŠ è½½æ–¹å¼:
      1. openvino_genai.LLMPipeline (æ¨è, ç›´æ¥æ”¯æŒ NPU)
      2. optimum-intel OVModelForCausalLM (å›é€€, ä»… CPU/GPU)

    æ¨èæ¨¡å‹ (INT4 å¯¹ç§°é‡åŒ–):
      - Qwen2.5-1.5B-Instruct  â†’ ä¸­æ–‡ä¼˜ç§€, ~1GB
      - MiniCPM-1B-sft          â†’ ç«¯ä¾§è®¾è®¡, ~0.6GB
      - Phi-3-mini-4k-instruct  â†’ æ¨ç†å¼º, ä¸­æ–‡å¼±
    """

    def __init__(self):
        self._pipe = None         # openvino_genai.LLMPipeline
        self._model = None        # OVModelForCausalLM (fallback)
        self._tokenizer = None    # transformers tokenizer (fallback)
        self._backend_type = None # "genai" | "optimum"
        self._device_used = None  # å®é™…ä½¿ç”¨çš„è®¾å¤‡

    def load(self) -> bool:
        model_path = llm_cfg.local_model_path
        device_chain = llm_cfg.local_device_fallback

        if not Path(model_path).exists():
            logger.warning(f"[LLM-Local] æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return False

        # æŒ‰é™çº§é“¾ä¾æ¬¡å°è¯•: NPU â†’ GPU â†’ CPU
        for device in device_chain:
            if self._try_load_genai(model_path, device):
                self._device_used = device
                # NPU é¢„çƒ­ (é¢„ç¼–è¯‘è®¡ç®—å›¾, é¿å…é¦–æ¬¡é—®ç­”å¡é¡¿)
                if llm_cfg.npu_warm_up and device == "NPU":
                    self._warm_up()
                return True

        # æ‰€æœ‰ genai è®¾å¤‡éƒ½å¤±è´¥, ç”¨ optimum-intel (ä»… CPU/GPU)
        if self._try_load_optimum(model_path, "CPU"):
            self._device_used = "CPU"
            return True

        logger.error("[LLM-Local] æ‰€æœ‰åŠ è½½æ–¹å¼å‡å¤±è´¥")
        return False

    def _try_load_genai(self, model_path: str, device: str) -> bool:
        """å°è¯•ç”¨ openvino_genai.LLMPipeline åŠ è½½åˆ°æŒ‡å®šè®¾å¤‡"""
        try:
            import openvino_genai as ov_genai

            cache_dir = str(Path(model_path) / llm_cfg.npu_cache_dir)

            pipeline_config = {}
            if device == "NPU":
                pipeline_config = {
                    "MAX_PROMPT_LEN": 1024,
                    "MIN_RESPONSE_LEN": 256,
                    "NPUW_CACHE_DIR": cache_dir,
                }

            logger.info(f"[LLM-Local] å°è¯• openvino_genai: {model_path} â†’ {device}")
            self._pipe = ov_genai.LLMPipeline(model_path, device, pipeline_config)
            self._backend_type = "genai"
            logger.info(f"[LLM-Local] openvino_genai åŠ è½½æˆåŠŸ (device={device})")
            return True
        except ImportError:
            logger.info("[LLM-Local] openvino_genai æœªå®‰è£…")
            return False
        except Exception as e:
            logger.warning(f"[LLM-Local] openvino_genai {device} åŠ è½½å¤±è´¥: {e}")
            self._pipe = None
            return False

    def _warm_up(self):
        """NPU é¢„çƒ­: å‘é€çŸ­ prompt é¢„ç¼–è¯‘è®¡ç®—å›¾"""
        try:
            import openvino_genai as ov_genai
            logger.info("[LLM-Local] NPU é¢„çƒ­ä¸­ (é¦–æ¬¡ç¼–è¯‘å¯èƒ½éœ€è¦ 5-15s)...")
            config = ov_genai.GenerationConfig()
            config.max_new_tokens = 2
            config.do_sample = False
            self._pipe.generate("ä½ å¥½", config)
            logger.info("[LLM-Local] NPU é¢„çƒ­å®Œæˆ")
        except Exception as e:
            logger.warning(f"[LLM-Local] NPU é¢„çƒ­å¤±è´¥ (ä¸å½±å“åç»­ä½¿ç”¨): {e}")

    def _try_load_optimum(self, model_path: str, device: str) -> bool:
        """å›é€€: ç”¨ optimum-intel åŠ è½½"""
        try:
            from optimum.intel.openvino import OVModelForCausalLM
            from transformers import AutoTokenizer

            # optimum çš„ device æ ¼å¼ä¸åŒ
            ov_device = "GPU" if device == "GPU" else "CPU"
            logger.info(f"[LLM-Local] ä½¿ç”¨ optimum-intel åŠ è½½: {model_path} â†’ {ov_device}")
            self._model = OVModelForCausalLM.from_pretrained(model_path, device=ov_device)
            self._tokenizer = AutoTokenizer.from_pretrained(model_path)
            self._backend_type = "optimum"
            logger.info("[LLM-Local] âœ… optimum-intel åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            logger.warning(f"[LLM-Local] optimum-intel åŠ è½½å¤±è´¥: {e}")
            return False

    def is_ready(self) -> bool:
        if self._backend_type == "genai":
            return self._pipe is not None
        elif self._backend_type == "optimum":
            return self._model is not None and self._tokenizer is not None
        return False

    def generate(self, system_prompt: str, user_message: str) -> str:
        if not self.is_ready():
            return "[Error] Local LLM not loaded."

        if self._backend_type == "genai":
            return self._generate_genai(system_prompt, user_message)
        else:
            return self._generate_optimum(system_prompt, user_message)

    def _generate_genai(self, system_prompt: str, user_message: str) -> str:
        """ä½¿ç”¨ openvino_genai LLMPipeline ç”Ÿæˆ"""
        try:
            # GenAI Pipeline ä½¿ç”¨çº¯æ–‡æœ¬ prompt (ä¸èµ° chat template, æ‰‹åŠ¨æ‹¼æ¥)
            prompt = (
                f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
                f"<|im_start|>user\n{user_message}<|im_end|>\n"
                f"<|im_start|>assistant\n"
            )

            import openvino_genai as ov_genai
            config = ov_genai.GenerationConfig()
            config.max_new_tokens = llm_cfg.max_tokens
            # NPU ç›®å‰ä»…æ”¯æŒ greedy decoding
            config.do_sample = False

            result = self._pipe.generate(prompt, config)
            return result.strip()
        except Exception as e:
            logger.error(f"[LLM-Local-GenAI] ç”Ÿæˆå¤±è´¥: {e}")
            return f"[Local Error] {e}"

    def _generate_optimum(self, system_prompt: str, user_message: str) -> str:
        """ä½¿ç”¨ optimum-intel ç”Ÿæˆ"""
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message},
            ]
            input_text = self._tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            input_ids = self._tokenizer(
                input_text, return_tensors="pt"
            ).input_ids.to(self._model.device)

            output = self._model.generate(
                input_ids,
                max_new_tokens=llm_cfg.max_tokens,
                temperature=llm_cfg.temperature,
            )
            answer = self._tokenizer.decode(output[0], skip_special_tokens=True)
            # å°è¯•å¤šç§åˆ†éš”ç¬¦æå– assistant å›å¤
            for sep in ("<|im_start|>assistant\n", "<|assistant|>", "assistant\n"):
                if sep in answer:
                    return answer.split(sep)[-1].strip()
            return answer.strip()
        except Exception as e:
            logger.error(f"[LLM-Local-Optimum] ç”Ÿæˆå¤±è´¥: {e}")
            return f"[Local Error] {e}"


# ====================================================================
# ç¬¬ä¸‰çº§: è§„åˆ™å¼•æ“åç«¯ (é›¶ä¾èµ–, ç¦»çº¿å…œåº•)
# ====================================================================

class RuleBasedBackend(LLMBackend):
    """
    åŸºäºé¢†åŸŸè§„åˆ™çš„æ¨¡æ¿å›å¤å¼•æ“

    å½“äº‘ç«¯ä¸å¯ç”¨ä¸”æœ¬åœ°æ¨¡å‹ä¹Ÿå¤±è´¥æ—¶, ä½¿ç”¨æ­¤åç«¯æä¾›åŸºæœ¬çš„ç”µè·¯åˆ†æèƒ½åŠ›ã€‚
    ä¸éœ€è¦ä»»ä½• ML ä¾èµ–, çº¯ Python å­—ç¬¦ä¸²å¤„ç†ã€‚

    èƒ½åŠ›èŒƒå›´:
    - è§£è¯»ç”µè·¯ç½‘è¡¨, å›ç­” "XXè¿åœ¨å“ªé‡Œ"
    - è¯†åˆ«å¸¸è§æ¥çº¿é”™è¯¯å¹¶ç»™å‡ºæç¤º
    - å›ç­”å…ƒä»¶åŸºç¡€çŸ¥è¯† (ç¦»çº¿çŸ¥è¯†åº“)
    - é¢åŒ…æ¿ä½¿ç”¨æŒ‡å¯¼
    - å¸¸è§å®éªŒç”µè·¯è¯Šæ–­
    """

    # å…ƒä»¶åŸºç¡€çŸ¥è¯†åº“ (æ‰©å……ç‰ˆ)
    KNOWLEDGE_BASE = {
        "RESISTOR": (
            "ç”µé˜»å™¨: é™åˆ¶ç”µæµ/åˆ†å‹ã€‚æ— ææ€§, å¯åŒå‘å®‰è£…ã€‚\n"
            "è‰²ç¯è¯»æ•°: ä»å·¦åˆ°å³è¯»å–è‰²ç¯å€¼, æœ€åä¸€ç¯ä¸ºè¯¯å·®ã€‚\n"
            "å¸¸ç”¨å€¼: 220Î©(LEDé™æµ), 1kÎ©(é€šç”¨), 10kÎ©(ä¸Šæ‹‰/ä¸‹æ‹‰), 4.7kÎ©(I2C)ã€‚\n"
            "åŠŸç‡: é¢åŒ…æ¿å¸¸ç”¨ 1/4W, è¶…è¿‡é¢å®šåŠŸç‡ä¼šçƒ§æ¯ã€‚"
        ),
        "LED": (
            "å‘å…‰äºŒæç®¡: æœ‰ææ€§! é•¿è„šä¸ºé˜³æ(+), çŸ­è„šä¸ºé˜´æ(-)ã€‚\n"
            "å¿…é¡»ä¸²è”é™æµç”µé˜», å¦åˆ™ç¬é—´çƒ§æ¯ã€‚\n"
            "é™æµç”µé˜»è®¡ç®—: R = (Vcc - Vf) / If\n"
            "  çº¢è‰²LED: Vfâ‰ˆ2.0V, å»ºè®® 220Î©(5V) æˆ– 100Î©(3.3V)\n"
            "  ç™½/è“LED: Vfâ‰ˆ3.2V, å»ºè®® 100Î©(5V)\n"
            "  ç»¿è‰²LED: Vfâ‰ˆ2.2V, å»ºè®® 150Î©(5V)\n"
            "å…¸å‹å·¥ä½œç”µæµ: 10-20mAã€‚"
        ),
        "DIODE": (
            "äºŒæç®¡: æœ‰ææ€§! æœ‰é“¶è‰²/ç™½è‰²è‰²ç¯çš„ä¸€ç«¯ä¸ºé˜´æ(-)ã€‚\n"
            "ç”µæµåªèƒ½ä»é˜³æ(+)æµå‘é˜´æ(-)ã€‚\n"
            "å¸¸è§å‹å·: 1N4148(ä¿¡å·), 1N4007(æ•´æµ), 1N5819(è‚–ç‰¹åŸº)ã€‚\n"
            "æ­£å‘å‹é™: ç¡…ç®¡çº¦0.7V, è‚–ç‰¹åŸºçº¦0.3Vã€‚"
        ),
        "CAPACITOR": (
            "ç”µå®¹å™¨:\n"
            "  ç”µè§£ç”µå®¹: æœ‰ææ€§! é•¿è„šä¸ºæ­£æ, çŸ­è„šä¸ºè´Ÿæ, ç½ä½“æœ‰ç™½è‰²æ ‡è®°ä¸ºè´Ÿæã€‚\n"
            "    åæ¥å¯èƒ½çˆ†è£‚! æ³¨æ„è€å‹å€¼, å®é™…ç”µå‹ä¸å¾—è¶…è¿‡æ ‡ç§°è€å‹çš„80%ã€‚\n"
            "  ç“·ç‰‡/ç‹¬çŸ³ç”µå®¹: æ— ææ€§ã€‚æ ‡æ³¨å¦‚ 104 = 100nF = 0.1Î¼Fã€‚\n"
            "å¸¸ç”¨æ»¤æ³¢æ­é…: ç”µæºå¤„å¹¶è” 100Î¼F(ç”µè§£) + 0.1Î¼F(ç“·ç‰‡)ã€‚"
        ),
        "WIRE": "å¯¼çº¿/è·³çº¿: æ— ææ€§, ç”¨äºè¿æ¥é¢åŒ…æ¿ä¸åŒè¡Œã€‚é€‰ç”¨åˆé€‚é•¿åº¦é¿å…æ··ä¹±ã€‚",
        "Push_Button": (
            "æŒ‰é’®å¼€å…³ (è½»è§¦å¼€å…³):\n"
            "  å››è„šæŒ‰é’®: å¯¹è§’ä¸¤è„šå§‹ç»ˆå¯¼é€š, æŒ‰ä¸‹æ—¶å››è„šå…¨å¯¼é€šã€‚\n"
            "  é¢åŒ…æ¿å®‰è£…: æ¨ªè·¨ä¸­é—´æ²Ÿæ§½, æ¯ä¾§å ä¸¤è¡Œã€‚\n"
            "  æ¶ˆæŠ–: ç¡¬ä»¶(å¹¶è”0.1Î¼F) æˆ– è½¯ä»¶(å»¶æ—¶10-50ms)ã€‚\n"
            "  ä¸Šæ‹‰æ¥æ³•: Vcc â†’ 10kÎ©ç”µé˜» â†’ å¼•è„š â†’ æŒ‰é’® â†’ GNDã€‚"
        ),
        "TRANSISTOR": (
            "ä¸‰æç®¡(BJT): TO-92å°è£…, å¹³é¢æœè‡ªå·±æ—¶å¼•è„šå®šä¹‰å–å†³äºå‹å·ã€‚\n"
            "  8050(NPN): E-B-C (å·¦åˆ°å³, å¹³é¢æœè‡ªå·±)ã€‚\n"
            "  8550(PNP): E-B-C (å·¦åˆ°å³, å¹³é¢æœè‡ªå·±)ã€‚\n"
            "  2N2222(NPN): E-B-Cã€‚ 2N3904(NPN): E-B-Cã€‚\n"
            "åŸºæé™æµç”µé˜»: é€šå¸¸ 1kÎ©-10kÎ©ã€‚\n"
            "æ”¾å¤§åŒºæ¡ä»¶: Vbeâ‰ˆ0.7V(å¯¼é€š), Vce > Vce(sat)ã€‚"
        ),
        "NPN": (
            "NPNä¸‰æç®¡:\n"
            "  åŸºæ(B)é«˜ç”µå¹³ â†’ å¯¼é€š, ç”µæµä»Cæµå‘Eã€‚\n"
            "  å¼€å…³ç”¨æ³•: åŸºæä¸²ç”µé˜»æ¥æ§åˆ¶ä¿¡å·, é›†ç”µææ¥è´Ÿè½½å’ŒVcc, å‘å°„ææ¥GNDã€‚\n"
            "  å¸¸è§å‹å·: 8050(Icâ‰¤0.5A), 2N2222(Icâ‰¤0.8A), 2N3904(Icâ‰¤0.2A)ã€‚"
        ),
        "PNP": (
            "PNPä¸‰æç®¡:\n"
            "  åŸºæ(B)ä½ç”µå¹³ â†’ å¯¼é€š, ç”µæµä»Eæµå‘Cã€‚\n"
            "  é«˜ç«¯å¼€å…³: å‘å°„ææ¥Vcc, é›†ç”µææ¥è´Ÿè½½, åŸºææ¥æ§åˆ¶ä¿¡å·ã€‚\n"
            "  å¸¸è§å‹å·: 8550(Icâ‰¤0.5A), 2N3906(Icâ‰¤0.2A)ã€‚"
        ),
        "IC": (
            "é›†æˆç”µè·¯(èŠ¯ç‰‡):\n"
            "  å®‰è£…: æ³¨æ„ç¼ºå£/åœ†ç‚¹æ ‡è®°, å¯¹å‡†Pin 1ã€‚åæ’ä¼šçƒ§æ¯!\n"
            "  ç”µæº: Vccå’ŒGNDå¿…é¡»æ­£ç¡®è¿æ¥, å»ºè®®åŠ 0.1Î¼Fæ—è·¯ç”µå®¹ã€‚\n"
            "  æ•£çƒ­: åŠŸç‡ICæ³¨æ„æ•£çƒ­, é¢åŒ…æ¿ä¸Šé¿å…é•¿æ—¶é—´å¤§ç”µæµã€‚"
        ),
        "OPAMP": (
            "è¿ç®—æ”¾å¤§å™¨(è¿æ”¾):\n"
            "  å¸¸è§å‹å·: LM358(åŒè¿æ”¾), LM741(å•è¿æ”¾), TL072(JFETè¾“å…¥)ã€‚\n"
            "  å¼•è„š: åç›¸è¾“å…¥(-), åŒç›¸è¾“å…¥(+), è¾“å‡º, V+, V-ã€‚\n"
            "  åŸºæœ¬ç”µè·¯:\n"
            "    åç›¸æ”¾å¤§: Av = -Rf/Rin\n"
            "    åŒç›¸æ”¾å¤§: Av = 1 + Rf/Rin\n"
            "    ç”µå‹è·Ÿéš: è¾“å‡ºç›´æ¥æ¥åç›¸è¾“å…¥, Av=1ã€‚\n"
            "  æ³¨æ„: å•ç”µæºè¿æ”¾(å¦‚LM358)è¾“å‡ºä¸èƒ½åˆ°è¾¾V-ã€‚"
        ),
        "555": (
            "555å®šæ—¶å™¨:\n"
            "  å¼•è„š: 1-GND, 2-TRIG, 3-OUT, 4-RESET, 5-CTRL, 6-THR, 7-DIS, 8-Vccã€‚\n"
            "  æ— ç¨³æ€(æ–¹æ³¢): t_high = 0.693*(Ra+Rb)*C, t_low = 0.693*Rb*Cã€‚\n"
            "  å•ç¨³æ€(å»¶æ—¶): t = 1.1*R*Cã€‚\n"
            "  VccèŒƒå›´: 4.5V-16V, CMOSç‰ˆ(7555)å¯ä½è‡³2Vã€‚"
        ),
    }

    # å¸¸è§é”™è¯¯æ£€æŸ¥è§„åˆ™ (æ‰©å……ç‰ˆ)
    ERROR_PATTERNS = {
        "LEDæ— é™æµç”µé˜»": "æ£€æµ‹åˆ°LEDç›´æ¥è·¨æ¥åœ¨ç”µæºè½¨ä¹‹é—´, ç¼ºå°‘é™æµç”µé˜»! å»ºè®®ä¸²è”220Î©-1kÎ©ç”µé˜»ã€‚",
        "äºŒæç®¡å¯èƒ½åæ¥": "äºŒæç®¡ææ€§å¯èƒ½æ¥åã€‚è¯·æ£€æŸ¥: é“¶è‰²/ç™½è‰²ç¯æ ‡è®°çš„ä¸€ç«¯åº”æ¥å‘ä½ç”µä½(GND)æ–¹å‘ã€‚",
        "çŸ­è·¯é£é™©": "æ£€æµ‹åˆ°ç”µæºæ­£è´Ÿæä¹‹é—´ç¼ºå°‘è´Ÿè½½, å­˜åœ¨çŸ­è·¯é£é™©!",
        "ç”µå®¹ææ€§": "ç”µè§£ç”µå®¹å¯èƒ½ææ€§æ¥åã€‚è¯·ç¡®è®¤: é•¿è„šæ¥æ­£æ, ç½ä½“ç™½è‰²æ ‡è®°ä¸€ä¾§æ¥è´Ÿæã€‚",
        "ä¸‰æç®¡å¼•è„š": "ä¸‰æç®¡å¼•è„šé¡ºåºé”™è¯¯å¯èƒ½å¯¼è‡´ä¸å·¥ä½œæˆ–çƒ§æ¯ã€‚è¯·ç¡®è®¤å‹å·å¹¶æ ¸å¯¹ E/B/C å¼•è„šã€‚",
        "æ‚¬ç©ºå¼•è„š": "æ£€æµ‹åˆ°æœªä½¿ç”¨çš„å…ƒä»¶å¼•è„šæ‚¬ç©º, å¯èƒ½å¯¼è‡´ä¸ç¨³å®šã€‚æ•°å­—ICå¼•è„šåº”æ¥é«˜/ä½ç”µå¹³ã€‚",
    }

    # é¢åŒ…æ¿ä½¿ç”¨çŸ¥è¯†
    BREADBOARD_KNOWLEDGE = (
        "é¢åŒ…æ¿ä½¿ç”¨æŒ‡å—:\n"
        "  - ç«–å‘5å­”ä¸€ç»„å¯¼é€š (a-e å’Œ f-j åˆ†åˆ«å¯¼é€š)\n"
        "  - ä¸­é—´æ²Ÿæ§½å°†ä¸Šä¸‹ä¸¤æ’éš”å¼€\n"
        "  - ä¸¤ä¾§ç”µæºè½¨æ¨ªå‘å¯¼é€š (çº¢+è“-)\n"
        "  - ICæ¨ªè·¨ä¸­é—´æ²Ÿæ§½å®‰è£…, ç¡®ä¿ä¸¤æ’å¼•è„šä¸çŸ­æ¥\n"
        "  - ç”µæºè½¨å¯èƒ½æœ‰æ–­ç‚¹ (ä¸­é—´ä½ç½®), éœ€ç”¨è·³çº¿æ¡¥æ¥\n"
        "  - é¿å…åœ¨åŒä¸€è¡Œæ’å…¥è¿‡å¤šå…ƒä»¶, æ¥è§¦ä¸è‰¯\n"
        "  - ç²—å¯¼çº¿ (>0.8mm) ä¼šæ’‘åå­”æ´"
    )

    def __init__(self):
        self._ready = True

    def load(self) -> bool:
        self._ready = True
        logger.info("[LLM-Rule] è§„åˆ™å¼•æ“å·²å°±ç»ª (ç¦»çº¿æ¨¡å¼)")
        return True

    def is_ready(self) -> bool:
        return self._ready

    def generate(self, system_prompt: str, user_message: str) -> str:
        """åŸºäºè§„åˆ™å’Œæ¨¡æ¿çš„å›å¤ç”Ÿæˆ"""
        question = user_message.strip()
        context = system_prompt  # system_prompt ä¸­åŒ…å«ç”µè·¯ç½‘è¡¨

        # 0. é¢åŒ…æ¿ä½¿ç”¨é—®é¢˜
        if any(kw in question for kw in ("é¢åŒ…æ¿", "breadboard", "æ€ä¹ˆç”¨", "æ€ä¹ˆæ’")):
            return self.BREADBOARD_KNOWLEDGE

        # 1. å…ƒä»¶çŸ¥è¯†æŸ¥è¯¢ (æ”¯æŒä¸­è‹±æ–‡åˆ«å)
        comp_aliases = {
            "ç”µé˜»": "RESISTOR", "é˜»": "RESISTOR",
            "LED": "LED", "å‘å…‰äºŒæç®¡": "LED", "ç¯": "LED",
            "äºŒæç®¡": "DIODE",
            "ç”µå®¹": "CAPACITOR", "å®¹": "CAPACITOR",
            "å¯¼çº¿": "WIRE", "è·³çº¿": "WIRE",
            "æŒ‰é’®": "Push_Button", "æŒ‰é”®": "Push_Button", "å¼€å…³": "Push_Button",
            "ä¸‰æç®¡": "TRANSISTOR", "BJT": "TRANSISTOR",
            "NPN": "NPN", "PNP": "PNP",
            "èŠ¯ç‰‡": "IC", "é›†æˆç”µè·¯": "IC",
            "è¿æ”¾": "OPAMP", "è¿ç®—æ”¾å¤§å™¨": "OPAMP", "LM358": "OPAMP", "741": "OPAMP",
            "555": "555", "å®šæ—¶å™¨": "555",
            "8050": "NPN", "8550": "PNP", "2N2222": "NPN", "2N3904": "NPN",
        }

        for alias, comp_type in comp_aliases.items():
            if alias.lower() in question.lower():
                info = self.KNOWLEDGE_BASE.get(comp_type, "")
                if info:
                    return self._format_knowledge_reply(comp_type, info, context)

        # åŸå§‹ class_name åŒ¹é…
        for comp_type, info in self.KNOWLEDGE_BASE.items():
            if comp_type.lower() in question.lower():
                return self._format_knowledge_reply(comp_type, info, context)

        # 2. è¿æ¥æŸ¥è¯¢ ("XXè¿åœ¨å“ªé‡Œ", "XXæ€ä¹ˆæ¥çš„")
        if any(kw in question for kw in ("è¿æ¥", "è¿åœ¨", "æ€ä¹ˆæ¥", "æ¥åœ¨", "è¿åˆ°", "æ¥çº¿")):
            return self._parse_connections_from_context(question, context)

        # 3. æ£€æŸ¥/éªŒè¯ç±»é—®é¢˜
        if any(kw in question for kw in ("æ£€æŸ¥", "æœ‰æ²¡æœ‰é”™", "å¯¹ä¸å¯¹", "æ­£ç¡®", "é—®é¢˜",
                                          "è¯Šæ–­", "æ’æŸ¥", "æ•…éšœ")):
            return self._check_circuit_issues(context)

        # 4. ç½‘è¡¨/å…ƒä»¶åˆ—è¡¨æŸ¥è¯¢
        if any(kw in question for kw in ("ç½‘è¡¨", "å…ƒä»¶", "åˆ—è¡¨", "å¤šå°‘ä¸ª", "å‡ ä¸ª")):
            return self._summarize_circuit(context)

        # 5. é€šç”¨å›å¤
        return (
            "æˆ‘æ˜¯ LabGuardian ç”µè·¯åŠ©æ‰‹ (ç¦»çº¿è§„åˆ™æ¨¡å¼)ã€‚\n"
            "ä½ å¯ä»¥é—®æˆ‘:\n"
            "  - å…ƒä»¶çŸ¥è¯†: 'ç”µé˜»æ€ä¹ˆçœ‹', 'LEDæ€ä¹ˆæ¥', '8050å¼•è„š'\n"
            "  - ç”µè·¯è¿æ¥: 'LEDè¿åœ¨å“ªé‡Œ', 'å½“å‰ç”µè·¯è¿æ¥æƒ…å†µ'\n"
            "  - ç”µè·¯æ£€æŸ¥: 'æ£€æŸ¥ä¸€ä¸‹æœ‰æ²¡æœ‰é”™'\n"
            "  - é¢åŒ…æ¿: 'é¢åŒ…æ¿æ€ä¹ˆç”¨'\n"
            "  - ç”µè·¯æ¦‚å†µ: 'å½“å‰æœ‰å¤šå°‘å…ƒä»¶'\n\n"
            "æœ¬åœ° AI æ¨¡å‹åŠ è½½åå¯è¿›è¡Œæ›´æ™ºèƒ½çš„é—®ç­”ã€‚"
        )

    def _format_knowledge_reply(self, comp_type: str, info: str, context: str) -> str:
        """æ ¼å¼åŒ–å…ƒä»¶çŸ¥è¯†å›å¤, ç»“åˆå½“å‰ç”µè·¯ä¸Šä¸‹æ–‡"""
        reply = f"ğŸ“– {info}\n"

        # ä» context ä¸­æ‰¾åˆ°è¯¥å…ƒä»¶çš„è¿æ¥ä¿¡æ¯
        connections = []
        for line in context.split("\n"):
            if comp_type in line.upper() and "Row" in line:
                connections.append(line.strip().lstrip("- "))

        if connections:
            reply += f"\nåœ¨å½“å‰ç”µè·¯ä¸­:\n"
            for c in connections[:5]:
                reply += f"  â€¢ {c}\n"

        return reply

    def _parse_connections_from_context(self, question: str, context: str) -> str:
        """ä»ç”µè·¯ä¸Šä¸‹æ–‡ä¸­è§£æè¿æ¥ä¿¡æ¯"""
        if not context or "æš‚æ— ç”µè·¯æ•°æ®" in context:
            return "å½“å‰æš‚æ— ç”µè·¯æ£€æµ‹æ•°æ®ã€‚è¯·ç¡®ä¿æ‘„åƒå¤´å¯¹å‡†é¢åŒ…æ¿, å¹¶å·²å®Œæˆæ ¡å‡†ã€‚"

        # æå– Component Connections éƒ¨åˆ†
        lines = []
        in_comp_section = False
        for line in context.split("\n"):
            if "Component Connections" in line:
                in_comp_section = True
                continue
            if in_comp_section and line.startswith("-"):
                lines.append(line.strip())
            elif in_comp_section and not line.strip():
                break

        if not lines:
            return "æœªæ£€æµ‹åˆ°å…ƒä»¶è¿æ¥ä¿¡æ¯ã€‚"

        return "å½“å‰æ£€æµ‹åˆ°çš„ç”µè·¯è¿æ¥:\n" + "\n".join(lines)

    def _check_circuit_issues(self, context: str) -> str:
        """åŸºäºè§„åˆ™æ£€æŸ¥å¸¸è§ç”µè·¯é”™è¯¯"""
        issues = []

        # æ£€æŸ¥ LED æ˜¯å¦æœ‰é™æµç”µé˜»
        if "LED" in context:
            has_resistor_near_led = "RESISTOR" in context
            if not has_resistor_near_led:
                issues.append(self.ERROR_PATTERNS["LEDæ— é™æµç”µé˜»"])

        # æ£€æŸ¥ææ€§é—®é¢˜ (ä» context ä¸­å¯»æ‰¾ææ€§æ ‡è®°)
        if "polarity: UNKNOWN" in context:
            issues.append("âš ï¸ éƒ¨åˆ†æœ‰ææ€§å…ƒä»¶çš„æ–¹å‘æ— æ³•ç¡®å®š, è¯·ç›®è§†æ£€æŸ¥ã€‚")

        if "æ¥å" in context or "REVERSE" in context:
            issues.append(self.ERROR_PATTERNS["äºŒæç®¡å¯èƒ½åæ¥"])

        if not issues:
            return "âœ… åŸºäºè§„åˆ™æ£€æŸ¥, æœªå‘ç°æ˜æ˜¾çš„æ¥çº¿é”™è¯¯ã€‚(æ³¨: ç¦»çº¿æ¨¡å¼ä»…æ£€æŸ¥å¸¸è§é—®é¢˜)"

        return "æ£€æŸ¥ç»“æœ:\n" + "\n".join(issues)

    def _summarize_circuit(self, context: str) -> str:
        """ä»ç”µè·¯ä¸Šä¸‹æ–‡ä¸­æå–å…ƒä»¶ç»Ÿè®¡å’Œè¿æ¥æ¦‚å†µ"""
        if not context or "æš‚æ— ç”µè·¯æ•°æ®" in context:
            return "å½“å‰æš‚æ— ç”µè·¯æ£€æµ‹æ•°æ®ã€‚è¯·ç¡®ä¿æ‘„åƒå¤´å¯¹å‡†é¢åŒ…æ¿, å¹¶å·²å®Œæˆæ ¡å‡†ã€‚"

        # ç»Ÿè®¡å„ç±»å…ƒä»¶æ•°é‡
        comp_counts = {}
        comp_names_cn = {
            "RESISTOR": "ç”µé˜»", "LED": "LED", "CAPACITOR": "ç”µå®¹",
            "DIODE": "äºŒæç®¡", "Wire": "å¯¼çº¿", "Push_Button": "æŒ‰é’®",
            "TRANSISTOR": "ä¸‰æç®¡", "NPN": "NPNä¸‰æç®¡", "PNP": "PNPä¸‰æç®¡",
            "IC": "èŠ¯ç‰‡", "OPAMP": "è¿æ”¾",
        }
        for comp_type in comp_names_cn:
            # æŒ‰è¡ŒåŒ¹é…, é¿å…å­ä¸²è¯¯è®¡ (å¦‚ "NPN" ä¸è¯¯åŒ¹é… "OPAMP")
            count = sum(1 for line in context.split("\n")
                        if comp_type in line and ("Row" in line or "row" in line))
            if count > 0:
                cn = comp_names_cn[comp_type]
                comp_counts[cn] = count

        if not comp_counts:
            return "æœªä»å½“å‰ç”µè·¯æ•°æ®ä¸­è¯†åˆ«åˆ°å…ƒä»¶ã€‚è¯·æ£€æŸ¥æ£€æµ‹ç»“æœã€‚"

        total = sum(comp_counts.values())
        summary = f"å½“å‰ç”µè·¯å…±æ£€æµ‹åˆ° {total} ä¸ªå…ƒä»¶:\n"
        for name, cnt in comp_counts.items():
            summary += f"  â€¢ {name}: {cnt} ä¸ª\n"

        # æå–ç½‘ç»œè¿æ¥æ•°
        net_count = context.count("Net_")
        if net_count > 0:
            summary += f"\nå…±æœ‰ {net_count} ä¸ªç”µæ°”ç½‘ç»œ (Net)ã€‚"

        return summary


class LLMEngine:
    """
    ç»Ÿä¸€ LLM æ¨ç†å¼•æ“ (v2) + RAG çŸ¥è¯†å¢å¼º

    ä¸‰çº§é™çº§ç­–ç•¥:
      1. Cloud  â†’ DeepSeek / Qwen ç­‰ (éœ€è”ç½‘, æœ€å¼º)
      2. Local  â†’ Qwen2.5 / MiniCPM ç­‰ via OpenVINO (ç¦»çº¿, ä¸­ç­‰)
      3. Rule   â†’ é¢†åŸŸè§„åˆ™æ¨¡æ¿å¼•æ“ (é›¶ä¾èµ–, å…œåº•)

    RAG å¢å¼º:
      - è‡ªåŠ¨æ£€ç´¢æœ¬åœ°çŸ¥è¯†åº“ (ChromaDB) ä¸­çš„ç›¸å…³ç‰‡æ®µ
      - å°†æ£€ç´¢ç»“æœæ³¨å…¥ system prompt, æå‡å›ç­”å‡†ç¡®æ€§
      - RAG ä¸å¯ç”¨æ—¶ä¼˜é›…é™çº§, ä¸å½±å“ä¸»æµç¨‹

    ä½¿ç”¨æ–¹å¼:
        engine = LLMEngine()
        engine.load()
        answer = engine.ask("8050ä¸‰æç®¡å¼•è„šæ€ä¹ˆæ¥ï¼Ÿ", circuit_context="...")
    """

    def __init__(self, enable_rag: bool = True):
        self.cloud = CloudLLMBackend()
        self.local = LocalLLMBackend()
        self.rules = RuleBasedBackend()
        self._active: Optional[LLMBackend] = None
        self._enable_rag = enable_rag
        self.rag = None  # RAGEngine instance (lazy init)

    def load(self) -> str:
        """
        åŠ è½½ LLM åç«¯ + RAG çŸ¥è¯†åº“, æŒ‰ä¼˜å…ˆçº§å°è¯•, è¿”å›åŠ è½½çŠ¶æ€æè¿°

        ç«èµ›æ¨¡å¼ (competition_mode=True):
          è·³è¿‡ Cloud, ç›´æ¥è¿›å…¥ Local é™çº§é“¾ (NPUâ†’GPUâ†’CPUâ†’Rules)
        """
        # 0. åˆå§‹åŒ– RAG çŸ¥è¯†åº“ (éé˜»å¡, å¤±è´¥ä¸å½±å“ LLM)
        rag_status = self._init_rag()

        mode_tag = "[ç«èµ›ç¦»çº¿æ¨¡å¼]" if llm_cfg.competition_mode else "[è”ç½‘æ¨¡å¼]"

        # 1. äº‘ç«¯ (ç«èµ›æ¨¡å¼è‡ªåŠ¨è·³è¿‡)
        if llm_cfg.use_cloud and self.cloud.load():
            self._active = self.cloud
            status = f"Cloud AI Ready: {llm_cfg.cloud_model_name}"
            logger.info(status)
            return f"{mode_tag} {status}\n{rag_status}"

        # 2. æœ¬åœ° OpenVINO æ¨¡å‹ (æŒ‰é™çº§é“¾ NPUâ†’GPUâ†’CPU)
        if self.local.load():
            self._active = self.local
            device = self.local._device_used or "?"
            status = f"Local AI Ready ({self.local._backend_type} on {device})"
            logger.info(status)
            return f"{mode_tag} {status}\n{rag_status}"

        # 3. è§„åˆ™å¼•æ“å…œåº• (æ°¸è¿œæˆåŠŸ)
        self.rules.load()
        self._active = self.rules
        status = "Rule-Based AI Ready (ç¦»çº¿è§„åˆ™æ¨¡å¼)"
        logger.info(status)
        return f"{mode_tag} {status}\n{rag_status}"

    def _init_rag(self) -> str:
        """åˆå§‹åŒ– RAG å¼•æ“ (ä¼˜é›…é™çº§)"""
        if not self._enable_rag:
            return "ğŸ“š RAG: å·²ç¦ç”¨"
        try:
            from ai.rag_engine import RAGEngine
            self.rag = RAGEngine()
            if self.rag.initialize():
                # å¦‚æœçŸ¥è¯†åº“ä¸ºç©º, è‡ªåŠ¨æ„å»ºç´¢å¼•
                if self.rag.doc_count == 0:
                    logger.info("[RAG] çŸ¥è¯†åº“ä¸ºç©º, è‡ªåŠ¨æ„å»ºç´¢å¼•...")
                    self.rag.build_index()
                count = self.rag.doc_count
                return f"ğŸ“š RAG Ready: {count} çŸ¥è¯†å—å·²åŠ è½½"
            else:
                self.rag = None
                return "ğŸ“š RAG: åˆå§‹åŒ–å¤±è´¥ (é™çº§ä¸ºæ— çŸ¥è¯†å¢å¼º)"
        except ImportError as e:
            logger.warning(f"[RAG] ä¾èµ–ç¼ºå¤±: {e}")
            self.rag = None
            return "ğŸ“š RAG: ä¾èµ–æœªå®‰è£… (pip install chromadb sentence-transformers)"
        except Exception as e:
            logger.warning(f"[RAG] åˆå§‹åŒ–å¼‚å¸¸: {e}")
            self.rag = None
            return f"ğŸ“š RAG: åˆå§‹åŒ–å¼‚å¸¸ ({e})"

    @property
    def is_ready(self) -> bool:
        return self._active is not None and self._active.is_ready()

    @property
    def backend_name(self) -> str:
        if self._active is self.cloud:
            return f"Cloud ({llm_cfg.cloud_model_name})"
        elif self._active is self.local:
            return f"Local-{self.local._backend_type}"
        elif self._active is self.rules:
            return "Rule-Based (Offline)"
        return "None"

    @property
    def is_offline_fallback(self) -> bool:
        """å½“å‰æ˜¯å¦åœ¨ä½¿ç”¨è§„åˆ™å¼•æ“å…œåº•"""
        return self._active is self.rules

    def switch_backend(self, backend: str) -> str:
        """
        æ‰‹åŠ¨åˆ‡æ¢åç«¯ (ç”¨äºè°ƒè¯•æˆ–æ¼”ç¤º)

        Args:
            backend: "cloud" | "local" | "rules"
        """
        if backend == "cloud" and self.cloud.is_ready():
            self._active = self.cloud
            return f"å·²åˆ‡æ¢åˆ° Cloud åç«¯"
        elif backend == "local" and self.local.is_ready():
            self._active = self.local
            return f"å·²åˆ‡æ¢åˆ° Local åç«¯"
        elif backend == "rules":
            self.rules.load()
            self._active = self.rules
            return "å·²åˆ‡æ¢åˆ° Rule-Based åç«¯"
        return f"åˆ‡æ¢å¤±è´¥: {backend} åç«¯ä¸å¯ç”¨"

    @property
    def rag_ready(self) -> bool:
        """RAG çŸ¥è¯†åº“æ˜¯å¦å¯ç”¨"""
        return self.rag is not None and self.rag.is_ready

    def ask(self, question: str, circuit_context: str = "") -> str:
        """
        å‘ AI æé—® (å¸¦ç”µè·¯ä¸Šä¸‹æ–‡ + RAG çŸ¥è¯†å¢å¼º)
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            circuit_context: å½“å‰ç”µè·¯çš„ç½‘è¡¨æè¿°
            
        Returns:
            AI å›å¤æ–‡æœ¬
        """
        if not self.is_ready:
            return "AI å¼•æ“æœªå°±ç»ªã€‚è¯·æ£€æŸ¥ API Key é…ç½®æˆ–æœ¬åœ°æ¨¡å‹ã€‚"

        # RAG æ£€ç´¢: ä»çŸ¥è¯†åº“è·å–ç›¸å…³ä¸Šä¸‹æ–‡
        rag_context = ""
        if self.rag_ready:
            try:
                rag_context = self.rag.get_context(question, top_k=3, min_score=0.35)
            except Exception as e:
                logger.warning(f"[RAG] æ£€ç´¢å¤±è´¥: {e}")

        system_prompt = self._build_system_prompt(circuit_context, rag_context)
        return self._active.generate(system_prompt, question)

    def ask_about_component(self, component_name: str, circuit_context: str = "") -> str:
        """å¿«æ·æ–¹æ³•ï¼šè¯¢é—®æŸä¸ªå…ƒä»¶çš„ä¿¡æ¯"""
        question = f"æˆ‘æ­£åœ¨çœ‹ä¸€ä¸ª {component_name}ï¼Œè¯·å‘Šè¯‰æˆ‘å®ƒåœ¨è¿™ä¸ªç”µè·¯ä¸­æ˜¯æ€ä¹ˆè¿æ¥çš„ï¼Ÿ"
        return self.ask(question, circuit_context)

    @staticmethod
    def _build_system_prompt(circuit_context: str, rag_context: str = "") -> str:
        # æ„å»ºçŸ¥è¯†åº“å‚è€ƒéƒ¨åˆ†
        knowledge_section = ""
        if rag_context:
            knowledge_section = f"""\n\nä½ è¿˜æ‹¥æœ‰ä»¥ä¸‹å‚è€ƒèµ„æ–™ (æ¥è‡ªæœ¬åœ°èŠ¯ç‰‡æ‰‹å†Œ/å®éªŒçŸ¥è¯†åº“):
{rag_context}
è¯·ä¼˜å…ˆå‚è€ƒè¿™äº›èµ„æ–™å›ç­”, å¹¶åœ¨å›ç­”ä¸­ç®€è¦æ³¨æ˜å‡ºå¤„ã€‚"""

        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå­å®éªŒå®¤åŠ©æ‰‹ (LabGuardian)ã€‚
ä½ æ‹¥æœ‰è®¡ç®—æœºè§†è§‰ç³»ç»Ÿæä¾›çš„å®æ—¶ç”µè·¯ç½‘è¡¨æ•°æ®ï¼š
{circuit_context if circuit_context else '(å½“å‰æš‚æ— ç”µè·¯æ•°æ®)'}
{knowledge_section}
è¯·åŸºäºæ­¤ç”µè·¯çŠ¶æ€å’Œå‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
- å¦‚æœè¢«é—®åŠè¿æ¥ï¼Œè¯·æ ¹æ® Net (ç½‘ç»œ) ä¿¡æ¯åˆ¤æ–­ã€‚
- Push_Button = æŒ‰é’®å¼€å…³, Wire = å¯¼çº¿ã€‚
- è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œåƒä¸€ä¸ªäººç±»åŠ©æ•™ä¸€æ ·è‡ªç„¶ã€‚
- å›ç­”ç®€æ´ï¼Œä¸è¦è¶…è¿‡ 300 å­—ã€‚"""
