"""
LLM æ¨ç†å¼•æ“æ¨¡å— (v2)
èŒè´£ï¼šæä¾›ç»Ÿä¸€çš„ AI é—®ç­”æ¥å£, è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åç«¯

åç«¯ä¼˜å…ˆçº§ (ä¸‰çº§é™çº§ç­–ç•¥):
  1. Cloud  â€” DeepSeek / Qwen ç­‰ OpenAI å…¼å®¹äº‘ç«¯ API (æœ€å¼º, éœ€è”ç½‘)
  2. Local  â€” Qwen2.5-1.5B / MiniCPM ç­‰å°æ¨¡å‹, é€šè¿‡ openvino_genai åœ¨ NPU/GPU ä¸Šè¿è¡Œ
  3. Rule   â€” åŸºäºç”µè·¯é¢†åŸŸè§„åˆ™çš„æ¨¡æ¿å¼•æ“ (é›¶ä¾èµ–, ç¦»çº¿å…œåº•)

æ¨¡å‹é€‰å‹å»ºè®® (Intel Core Ultra 5 225U Â· DK-2500):
  - é¦–é€‰: Qwen2.5-1.5B-Instruct (INT4) â€” ä¸­æ–‡æœ€å¥½çš„ 1B çº§æ¨¡å‹
  - æ¬¡é€‰: MiniCPM-1B-sft (INT4) â€” æ¸…å/é¢å£, ä¸“ä¸ºç«¯ä¾§è®¾è®¡
  - å¤‡é€‰: Phi-3-mini-4k-instruct (INT4) â€” æ¨ç†èƒ½åŠ›å¼º, ä¸­æ–‡ç•¥å¼±
  - å¼ƒç”¨: TinyLlama-1.1B â€” ä¸­æ–‡å‡ ä¹ä¸å¯ç”¨

è½¬æ¢å‘½ä»¤ (åœ¨æœ‰ç½‘ç¯å¢ƒæ‰§è¡Œä¸€æ¬¡):
  optimum-cli export openvino \\
    --model Qwen/Qwen2.5-1.5B-Instruct \\
    --weight-format int4 --sym --ratio 1.0 --group-size 128 \\
    --trust-remote-code \\
    models/qwen2.5_1.5b_ov

å‚è€ƒ:
  - OpenVINO GenAI NPU Guide: docs.openvino.ai/2024/learn-openvino/llm_inference_guide/genai-guide-npu.html
  - openvino_genai LLMPipeline: github.com/openvinotoolkit/openvino.genai
"""

import logging
import re
from abc import ABC, abstractmethod
from typing import Optional

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from config import llm as llm_cfg

logger = logging.getLogger(__name__)


class LLMBackend(ABC):
    """LLM åç«¯æŠ½è±¡åŸºç±»"""

    @abstractmethod
    def generate(self, system_prompt: str, user_message: str) -> str:
        """ç”Ÿæˆå›å¤"""
        ...

    @abstractmethod
    def is_ready(self) -> bool:
        """æ˜¯å¦å·²åŠ è½½å°±ç»ª"""
        ...


class CloudLLMBackend(LLMBackend):
    """äº‘ç«¯ API åç«¯ (DeepSeek / Moonshot / Qwen ç­‰ OpenAI å…¼å®¹æ¥å£)"""

    def __init__(self):
        self.client = None

    def load(self) -> bool:
        if not llm_cfg.is_cloud_ready:
            print("[LLM-Cloud] API Key æœªé…ç½®. è®¾ç½®ç¯å¢ƒå˜é‡ LG_API_KEY æˆ– DEEPSEEK_API_KEY")
            return False
        try:
            from openai import OpenAI
            self.client = OpenAI(
                api_key=llm_cfg.cloud_api_key,
                base_url=llm_cfg.cloud_base_url,
            )
            print(f"[LLM-Cloud] å·²è¿æ¥: {llm_cfg.cloud_model_name} @ {llm_cfg.cloud_base_url}")
            return True
        except Exception as e:
            print(f"[LLM-Cloud] è¿æ¥å¤±è´¥: {e}")
            return False

    def is_ready(self) -> bool:
        return self.client is not None

    def generate(self, system_prompt: str, user_message: str) -> str:
        if not self.is_ready():
            return "[Error] Cloud LLM not ready."
        try:
            response = self.client.chat.completions.create(
                model=llm_cfg.cloud_model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message},
                ],
                max_tokens=llm_cfg.max_tokens,
                temperature=llm_cfg.temperature,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"[Cloud Error] {e}"


class LocalLLMBackend(LLMBackend):
    """
    æœ¬åœ° LLM åç«¯ (OpenVINO GenAI)

    æ”¯æŒä¸¤ç§åŠ è½½æ–¹å¼:
    1. openvino_genai.LLMPipeline (æ¨è, æ”¯æŒ NPU)
    2. optimum-intel OVModelForCausalLM (å›é€€)

    æ¨èæ¨¡å‹ (INT4 å¯¹ç§°é‡åŒ–):
      - Qwen2.5-1.5B-Instruct  â†’ ä¸­æ–‡ä¼˜ç§€, ~1GB
      - MiniCPM-1B-sft          â†’ ç«¯ä¾§è®¾è®¡, ~0.6GB
      - Phi-3-mini-4k-instruct  â†’ æ¨ç†å¼º, ä¸­æ–‡å¼±
    """

    def __init__(self):
        self._pipe = None         # openvino_genai.LLMPipeline
        self._model = None        # OVModelForCausalLM (fallback)
        self._tokenizer = None    # transformers tokenizer (fallback)
        self._backend_type = None # "genai" | "optimum"

    def load(self) -> bool:
        model_path = llm_cfg.local_model_path
        device = llm_cfg.local_device

        if not Path(model_path).exists():
            logger.warning(f"[LLM-Local] æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return False

        # ç­–ç•¥ 1: ä¼˜å…ˆç”¨ openvino_genai (æ›´è½»é‡, æ”¯æŒ NPU, æ—  transformers ä¾èµ–)
        if self._try_load_genai(model_path, device):
            return True

        # ç­–ç•¥ 2: å›é€€åˆ° optimum-intel
        if self._try_load_optimum(model_path, device):
            return True

        logger.error("[LLM-Local] æ‰€æœ‰åŠ è½½æ–¹å¼å‡å¤±è´¥")
        return False

    def _try_load_genai(self, model_path: str, device: str) -> bool:
        """å°è¯•ç”¨ openvino_genai.LLMPipeline åŠ è½½"""
        try:
            import openvino_genai as ov_genai

            pipeline_config = {
                "MAX_PROMPT_LEN": 1024,
                "MIN_RESPONSE_LEN": 256,
                "NPUW_CACHE_DIR": str(Path(model_path) / ".npucache"),
            }

            logger.info(f"[LLM-Local] ä½¿ç”¨ openvino_genai åŠ è½½: {model_path} â†’ {device}")
            self._pipe = ov_genai.LLMPipeline(model_path, device, pipeline_config)
            self._backend_type = "genai"
            logger.info(f"[LLM-Local] âœ… openvino_genai LLMPipeline åŠ è½½æˆåŠŸ (device={device})")
            return True
        except ImportError:
            logger.info("[LLM-Local] openvino_genai æœªå®‰è£…, å°è¯• optimum-intel")
            return False
        except Exception as e:
            logger.warning(f"[LLM-Local] openvino_genai åŠ è½½å¤±è´¥: {e}")
            return False

    def _try_load_optimum(self, model_path: str, device: str) -> bool:
        """å›é€€: ç”¨ optimum-intel åŠ è½½"""
        try:
            from optimum.intel.openvino import OVModelForCausalLM
            from transformers import AutoTokenizer

            # optimum çš„ device æ ¼å¼ä¸åŒ
            ov_device = "GPU" if device == "GPU" else "CPU"
            logger.info(f"[LLM-Local] ä½¿ç”¨ optimum-intel åŠ è½½: {model_path} â†’ {ov_device}")
            self._model = OVModelForCausalLM.from_pretrained(model_path, device=ov_device)
            self._tokenizer = AutoTokenizer.from_pretrained(model_path)
            self._backend_type = "optimum"
            logger.info("[LLM-Local] âœ… optimum-intel åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            logger.warning(f"[LLM-Local] optimum-intel åŠ è½½å¤±è´¥: {e}")
            return False

    def is_ready(self) -> bool:
        if self._backend_type == "genai":
            return self._pipe is not None
        elif self._backend_type == "optimum":
            return self._model is not None and self._tokenizer is not None
        return False

    def generate(self, system_prompt: str, user_message: str) -> str:
        if not self.is_ready():
            return "[Error] Local LLM not loaded."

        if self._backend_type == "genai":
            return self._generate_genai(system_prompt, user_message)
        else:
            return self._generate_optimum(system_prompt, user_message)

    def _generate_genai(self, system_prompt: str, user_message: str) -> str:
        """ä½¿ç”¨ openvino_genai LLMPipeline ç”Ÿæˆ"""
        try:
            # GenAI Pipeline ä½¿ç”¨çº¯æ–‡æœ¬ prompt (ä¸èµ° chat template, æ‰‹åŠ¨æ‹¼æ¥)
            prompt = (
                f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
                f"<|im_start|>user\n{user_message}<|im_end|>\n"
                f"<|im_start|>assistant\n"
            )

            import openvino_genai as ov_genai
            config = ov_genai.GenerationConfig()
            config.max_new_tokens = llm_cfg.max_tokens
            # NPU ç›®å‰ä»…æ”¯æŒ greedy decoding
            config.do_sample = False

            result = self._pipe.generate(prompt, config)
            return result.strip()
        except Exception as e:
            logger.error(f"[LLM-Local-GenAI] ç”Ÿæˆå¤±è´¥: {e}")
            return f"[Local Error] {e}"

    def _generate_optimum(self, system_prompt: str, user_message: str) -> str:
        """ä½¿ç”¨ optimum-intel ç”Ÿæˆ"""
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message},
            ]
            input_text = self._tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            input_ids = self._tokenizer(
                input_text, return_tensors="pt"
            ).input_ids.to(self._model.device)

            output = self._model.generate(
                input_ids,
                max_new_tokens=llm_cfg.max_tokens,
                temperature=llm_cfg.temperature,
            )
            answer = self._tokenizer.decode(output[0], skip_special_tokens=True)
            # å°è¯•å¤šç§åˆ†éš”ç¬¦æå– assistant å›å¤
            for sep in ("<|im_start|>assistant\n", "<|assistant|>", "assistant\n"):
                if sep in answer:
                    return answer.split(sep)[-1].strip()
            return answer.strip()
        except Exception as e:
            logger.error(f"[LLM-Local-Optimum] ç”Ÿæˆå¤±è´¥: {e}")
            return f"[Local Error] {e}"


# ====================================================================
# ç¬¬ä¸‰çº§: è§„åˆ™å¼•æ“åç«¯ (é›¶ä¾èµ–, ç¦»çº¿å…œåº•)
# ====================================================================

class RuleBasedBackend(LLMBackend):
    """
    åŸºäºé¢†åŸŸè§„åˆ™çš„æ¨¡æ¿å›å¤å¼•æ“

    å½“äº‘ç«¯ä¸å¯ç”¨ä¸”æœ¬åœ°æ¨¡å‹ä¹Ÿå¤±è´¥æ—¶, ä½¿ç”¨æ­¤åç«¯æä¾›åŸºæœ¬çš„ç”µè·¯åˆ†æèƒ½åŠ›ã€‚
    ä¸éœ€è¦ä»»ä½• ML ä¾èµ–, çº¯ Python å­—ç¬¦ä¸²å¤„ç†ã€‚

    èƒ½åŠ›èŒƒå›´:
    - è§£è¯»ç”µè·¯ç½‘è¡¨, å›ç­” "XXè¿åœ¨å“ªé‡Œ"
    - è¯†åˆ«å¸¸è§æ¥çº¿é”™è¯¯å¹¶ç»™å‡ºæç¤º
    - å›ç­”å…ƒä»¶åŸºç¡€çŸ¥è¯† (ç¦»çº¿çŸ¥è¯†åº“)
    """

    # å…ƒä»¶åŸºç¡€çŸ¥è¯†åº“
    KNOWLEDGE_BASE = {
        "RESISTOR": "ç”µé˜»å™¨: é™åˆ¶ç”µæµ/åˆ†å‹ã€‚æ— ææ€§, å¯åŒå‘å®‰è£…ã€‚è‰²ç¯è¯»æ•°ä»å·¦åˆ°å³ã€‚",
        "LED": "å‘å…‰äºŒæç®¡: æœ‰ææ€§! é•¿è„šä¸ºé˜³æ(+), çŸ­è„šä¸ºé˜´æ(-)ã€‚å¿…é¡»ä¸²è”é™æµç”µé˜»(é€šå¸¸220Î©-1kÎ©)ã€‚",
        "DIODE": "äºŒæç®¡: æœ‰ææ€§! æœ‰é“¶è‰²/ç™½è‰²è‰²ç¯çš„ä¸€ç«¯ä¸ºé˜´æ(-)ã€‚ç”µæµåªèƒ½ä»é˜³ææµå‘é˜´æã€‚",
        "CAPACITOR": "ç”µå®¹å™¨: ç”µè§£ç”µå®¹æœ‰ææ€§(é•¿è„šæ­£æ)! é™¶ç“·ç”µå®¹æ— ææ€§ã€‚æ³¨æ„è€å‹å€¼ã€‚",
        "WIRE": "å¯¼çº¿/è·³çº¿: æ— ææ€§, ç”¨äºè¿æ¥é¢åŒ…æ¿ä¸åŒè¡Œã€‚",
        "Push_Button": "æŒ‰é’®å¼€å…³: æŒ‰ä¸‹å¯¼é€š, æ¾å¼€æ–­å¼€ã€‚å››è„šæŒ‰é’®æ³¨æ„å¯¹è§’è¿é€šã€‚",
        "TRANSISTOR": "ä¸‰æç®¡(BJT): TO-92å°è£…, å¹³é¢æœè‡ªå·±æ—¶å¼•è„šä»å·¦åˆ°å³ä¸º E/B/Cã€‚æ³¨æ„å‹å·(NPN/PNP)ã€‚",
        "NPN": "NPNä¸‰æç®¡: åŸºæ(B)é«˜ç”µå¹³æ—¶å¯¼é€š, ç”µæµä»é›†ç”µæ(C)æµå‘å‘å°„æ(E)ã€‚",
        "PNP": "PNPä¸‰æç®¡: åŸºæ(B)ä½ç”µå¹³æ—¶å¯¼é€š, ç”µæµä»å‘å°„æ(E)æµå‘é›†ç”µæ(C)ã€‚",
    }

    # å¸¸è§é”™è¯¯æ£€æŸ¥è§„åˆ™
    ERROR_PATTERNS = {
        "LEDæ— é™æµç”µé˜»": "âš ï¸ æ£€æµ‹åˆ°LEDç›´æ¥è·¨æ¥åœ¨ç”µæºè½¨ä¹‹é—´, ç¼ºå°‘é™æµç”µé˜»! å»ºè®®ä¸²è”220Î©-1kÎ©ç”µé˜»ã€‚",
        "äºŒæç®¡å¯èƒ½åæ¥": "âš ï¸ äºŒæç®¡ææ€§å¯èƒ½æ¥åã€‚è¯·æ£€æŸ¥: é“¶è‰²/ç™½è‰²ç¯æ ‡è®°çš„ä¸€ç«¯åº”æ¥å‘ä½ç”µä½(GND)æ–¹å‘ã€‚",
        "çŸ­è·¯é£é™©": "ğŸ”´ æ£€æµ‹åˆ°ç”µæºæ­£è´Ÿæä¹‹é—´ç¼ºå°‘è´Ÿè½½, å­˜åœ¨çŸ­è·¯é£é™©!",
    }

    def __init__(self):
        self._ready = True

    def load(self) -> bool:
        self._ready = True
        logger.info("[LLM-Rule] è§„åˆ™å¼•æ“å·²å°±ç»ª (ç¦»çº¿æ¨¡å¼)")
        return True

    def is_ready(self) -> bool:
        return self._ready

    def generate(self, system_prompt: str, user_message: str) -> str:
        """åŸºäºè§„åˆ™å’Œæ¨¡æ¿çš„å›å¤ç”Ÿæˆ"""
        question = user_message.strip()
        context = system_prompt  # system_prompt ä¸­åŒ…å«ç”µè·¯ç½‘è¡¨

        # 1. å…ƒä»¶çŸ¥è¯†æŸ¥è¯¢
        for comp_type, info in self.KNOWLEDGE_BASE.items():
            if comp_type.lower() in question.lower():
                return self._format_knowledge_reply(comp_type, info, context)

        # 2. è¿æ¥æŸ¥è¯¢ ("XXè¿åœ¨å“ªé‡Œ", "XXæ€ä¹ˆæ¥çš„")
        if any(kw in question for kw in ("è¿æ¥", "è¿åœ¨", "æ€ä¹ˆæ¥", "æ¥åœ¨", "è¿åˆ°")):
            return self._parse_connections_from_context(question, context)

        # 3. æ£€æŸ¥/éªŒè¯ç±»é—®é¢˜
        if any(kw in question for kw in ("æ£€æŸ¥", "æœ‰æ²¡æœ‰é”™", "å¯¹ä¸å¯¹", "æ­£ç¡®", "é—®é¢˜")):
            return self._check_circuit_issues(context)

        # 4. é€šç”¨å›å¤
        return (
            "æˆ‘æ˜¯ LabGuardian ç”µè·¯åŠ©æ‰‹ (ç¦»çº¿æ¨¡å¼)ã€‚\n"
            "ä½ å¯ä»¥é—®æˆ‘:\n"
            "â€¢ æŸä¸ªå…ƒä»¶çš„åŸºç¡€çŸ¥è¯† (å¦‚ 'ç”µé˜»æ˜¯ä»€ä¹ˆ')\n"
            "â€¢ å½“å‰ç”µè·¯çš„è¿æ¥æƒ…å†µ (å¦‚ 'LEDè¿åœ¨å“ªé‡Œ')\n"
            "â€¢ ç”µè·¯æ£€æŸ¥ (å¦‚ 'æ£€æŸ¥ä¸€ä¸‹æœ‰æ²¡æœ‰é”™')\n\n"
            "æç¤º: è”ç½‘åå¯ä½¿ç”¨æ›´å¼ºå¤§çš„ AI é—®ç­”ã€‚"
        )

    def _format_knowledge_reply(self, comp_type: str, info: str, context: str) -> str:
        """æ ¼å¼åŒ–å…ƒä»¶çŸ¥è¯†å›å¤, ç»“åˆå½“å‰ç”µè·¯ä¸Šä¸‹æ–‡"""
        reply = f"ğŸ“– {info}\n"

        # ä» context ä¸­æ‰¾åˆ°è¯¥å…ƒä»¶çš„è¿æ¥ä¿¡æ¯
        connections = []
        for line in context.split("\n"):
            if comp_type in line.upper() and "Row" in line:
                connections.append(line.strip().lstrip("- "))

        if connections:
            reply += f"\nåœ¨å½“å‰ç”µè·¯ä¸­:\n"
            for c in connections[:5]:
                reply += f"  â€¢ {c}\n"

        return reply

    def _parse_connections_from_context(self, question: str, context: str) -> str:
        """ä»ç”µè·¯ä¸Šä¸‹æ–‡ä¸­è§£æè¿æ¥ä¿¡æ¯"""
        if not context or "æš‚æ— ç”µè·¯æ•°æ®" in context:
            return "å½“å‰æš‚æ— ç”µè·¯æ£€æµ‹æ•°æ®ã€‚è¯·ç¡®ä¿æ‘„åƒå¤´å¯¹å‡†é¢åŒ…æ¿, å¹¶å·²å®Œæˆæ ¡å‡†ã€‚"

        # æå– Component Connections éƒ¨åˆ†
        lines = []
        in_comp_section = False
        for line in context.split("\n"):
            if "Component Connections" in line:
                in_comp_section = True
                continue
            if in_comp_section and line.startswith("-"):
                lines.append(line.strip())
            elif in_comp_section and not line.strip():
                break

        if not lines:
            return "æœªæ£€æµ‹åˆ°å…ƒä»¶è¿æ¥ä¿¡æ¯ã€‚"

        return "å½“å‰æ£€æµ‹åˆ°çš„ç”µè·¯è¿æ¥:\n" + "\n".join(lines)

    def _check_circuit_issues(self, context: str) -> str:
        """åŸºäºè§„åˆ™æ£€æŸ¥å¸¸è§ç”µè·¯é”™è¯¯"""
        issues = []

        # æ£€æŸ¥ LED æ˜¯å¦æœ‰é™æµç”µé˜»
        if "LED" in context:
            has_resistor_near_led = "RESISTOR" in context
            if not has_resistor_near_led:
                issues.append(self.ERROR_PATTERNS["LEDæ— é™æµç”µé˜»"])

        # æ£€æŸ¥ææ€§é—®é¢˜ (ä» context ä¸­å¯»æ‰¾ææ€§æ ‡è®°)
        if "polarity: UNKNOWN" in context:
            issues.append("âš ï¸ éƒ¨åˆ†æœ‰ææ€§å…ƒä»¶çš„æ–¹å‘æ— æ³•ç¡®å®š, è¯·ç›®è§†æ£€æŸ¥ã€‚")

        if "æ¥å" in context or "REVERSE" in context:
            issues.append(self.ERROR_PATTERNS["äºŒæç®¡å¯èƒ½åæ¥"])

        if not issues:
            return "âœ… åŸºäºè§„åˆ™æ£€æŸ¥, æœªå‘ç°æ˜æ˜¾çš„æ¥çº¿é”™è¯¯ã€‚(æ³¨: ç¦»çº¿æ¨¡å¼ä»…æ£€æŸ¥å¸¸è§é—®é¢˜)"

        return "æ£€æŸ¥ç»“æœ:\n" + "\n".join(issues)


class LLMEngine:
    """
    ç»Ÿä¸€ LLM æ¨ç†å¼•æ“ (v2) + RAG çŸ¥è¯†å¢å¼º

    ä¸‰çº§é™çº§ç­–ç•¥:
      1. Cloud  â†’ DeepSeek / Qwen ç­‰ (éœ€è”ç½‘, æœ€å¼º)
      2. Local  â†’ Qwen2.5 / MiniCPM ç­‰ via OpenVINO (ç¦»çº¿, ä¸­ç­‰)
      3. Rule   â†’ é¢†åŸŸè§„åˆ™æ¨¡æ¿å¼•æ“ (é›¶ä¾èµ–, å…œåº•)

    RAG å¢å¼º:
      - è‡ªåŠ¨æ£€ç´¢æœ¬åœ°çŸ¥è¯†åº“ (ChromaDB) ä¸­çš„ç›¸å…³ç‰‡æ®µ
      - å°†æ£€ç´¢ç»“æœæ³¨å…¥ system prompt, æå‡å›ç­”å‡†ç¡®æ€§
      - RAG ä¸å¯ç”¨æ—¶ä¼˜é›…é™çº§, ä¸å½±å“ä¸»æµç¨‹

    ä½¿ç”¨æ–¹å¼:
        engine = LLMEngine()
        engine.load()
        answer = engine.ask("8050ä¸‰æç®¡å¼•è„šæ€ä¹ˆæ¥ï¼Ÿ", circuit_context="...")
    """

    def __init__(self, enable_rag: bool = True):
        self.cloud = CloudLLMBackend()
        self.local = LocalLLMBackend()
        self.rules = RuleBasedBackend()
        self._active: Optional[LLMBackend] = None
        self._enable_rag = enable_rag
        self.rag = None  # RAGEngine instance (lazy init)

    def load(self) -> str:
        """
        åŠ è½½ LLM åç«¯ + RAG çŸ¥è¯†åº“, æŒ‰ä¼˜å…ˆçº§å°è¯•, è¿”å›åŠ è½½çŠ¶æ€æè¿°
        """
        # 0. åˆå§‹åŒ– RAG çŸ¥è¯†åº“ (éé˜»å¡, å¤±è´¥ä¸å½±å“ LLM)
        rag_status = self._init_rag()

        # 1. ä¼˜å…ˆå°è¯•äº‘ç«¯
        if llm_cfg.use_cloud and self.cloud.load():
            self._active = self.cloud
            status = f"â˜ï¸ Cloud AI Ready: {llm_cfg.cloud_model_name}"
            logger.info(status)
            return f"{status}\n{rag_status}"

        # 2. æœ¬åœ° OpenVINO æ¨¡å‹
        if self.local.load():
            self._active = self.local
            status = f"ğŸ’» Local AI Ready ({self.local._backend_type})"
            logger.info(status)
            return f"{status}\n{rag_status}"

        # 3. è§„åˆ™å¼•æ“å…œåº• (æ°¸è¿œæˆåŠŸ)
        self.rules.load()
        self._active = self.rules
        status = "ğŸ“‹ Rule-Based AI Ready (ç¦»çº¿æ¨¡å¼)"
        logger.info(status)
        return f"{status}\n{rag_status}"

    def _init_rag(self) -> str:
        """åˆå§‹åŒ– RAG å¼•æ“ (ä¼˜é›…é™çº§)"""
        if not self._enable_rag:
            return "ğŸ“š RAG: å·²ç¦ç”¨"
        try:
            from ai.rag_engine import RAGEngine
            self.rag = RAGEngine()
            if self.rag.initialize():
                # å¦‚æœçŸ¥è¯†åº“ä¸ºç©º, è‡ªåŠ¨æ„å»ºç´¢å¼•
                if self.rag.doc_count == 0:
                    logger.info("[RAG] çŸ¥è¯†åº“ä¸ºç©º, è‡ªåŠ¨æ„å»ºç´¢å¼•...")
                    self.rag.build_index()
                count = self.rag.doc_count
                return f"ğŸ“š RAG Ready: {count} çŸ¥è¯†å—å·²åŠ è½½"
            else:
                self.rag = None
                return "ğŸ“š RAG: åˆå§‹åŒ–å¤±è´¥ (é™çº§ä¸ºæ— çŸ¥è¯†å¢å¼º)"
        except ImportError as e:
            logger.warning(f"[RAG] ä¾èµ–ç¼ºå¤±: {e}")
            self.rag = None
            return "ğŸ“š RAG: ä¾èµ–æœªå®‰è£… (pip install chromadb sentence-transformers)"
        except Exception as e:
            logger.warning(f"[RAG] åˆå§‹åŒ–å¼‚å¸¸: {e}")
            self.rag = None
            return f"ğŸ“š RAG: åˆå§‹åŒ–å¼‚å¸¸ ({e})"

    @property
    def is_ready(self) -> bool:
        return self._active is not None and self._active.is_ready()

    @property
    def backend_name(self) -> str:
        if self._active is self.cloud:
            return f"Cloud ({llm_cfg.cloud_model_name})"
        elif self._active is self.local:
            return f"Local-{self.local._backend_type}"
        elif self._active is self.rules:
            return "Rule-Based (Offline)"
        return "None"

    @property
    def is_offline_fallback(self) -> bool:
        """å½“å‰æ˜¯å¦åœ¨ä½¿ç”¨è§„åˆ™å¼•æ“å…œåº•"""
        return self._active is self.rules

    def switch_backend(self, backend: str) -> str:
        """
        æ‰‹åŠ¨åˆ‡æ¢åç«¯ (ç”¨äºè°ƒè¯•æˆ–æ¼”ç¤º)

        Args:
            backend: "cloud" | "local" | "rules"
        """
        if backend == "cloud" and self.cloud.is_ready():
            self._active = self.cloud
            return f"å·²åˆ‡æ¢åˆ° Cloud åç«¯"
        elif backend == "local" and self.local.is_ready():
            self._active = self.local
            return f"å·²åˆ‡æ¢åˆ° Local åç«¯"
        elif backend == "rules":
            self.rules.load()
            self._active = self.rules
            return "å·²åˆ‡æ¢åˆ° Rule-Based åç«¯"
        return f"åˆ‡æ¢å¤±è´¥: {backend} åç«¯ä¸å¯ç”¨"

    @property
    def rag_ready(self) -> bool:
        """RAG çŸ¥è¯†åº“æ˜¯å¦å¯ç”¨"""
        return self.rag is not None and self.rag.is_ready

    def ask(self, question: str, circuit_context: str = "") -> str:
        """
        å‘ AI æé—® (å¸¦ç”µè·¯ä¸Šä¸‹æ–‡ + RAG çŸ¥è¯†å¢å¼º)
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            circuit_context: å½“å‰ç”µè·¯çš„ç½‘è¡¨æè¿°
            
        Returns:
            AI å›å¤æ–‡æœ¬
        """
        if not self.is_ready:
            return "AI å¼•æ“æœªå°±ç»ªã€‚è¯·æ£€æŸ¥ API Key é…ç½®æˆ–æœ¬åœ°æ¨¡å‹ã€‚"

        # RAG æ£€ç´¢: ä»çŸ¥è¯†åº“è·å–ç›¸å…³ä¸Šä¸‹æ–‡
        rag_context = ""
        if self.rag_ready:
            try:
                rag_context = self.rag.get_context(question, top_k=3, min_score=0.35)
            except Exception as e:
                logger.warning(f"[RAG] æ£€ç´¢å¤±è´¥: {e}")

        system_prompt = self._build_system_prompt(circuit_context, rag_context)
        return self._active.generate(system_prompt, question)

    def ask_about_component(self, component_name: str, circuit_context: str = "") -> str:
        """å¿«æ·æ–¹æ³•ï¼šè¯¢é—®æŸä¸ªå…ƒä»¶çš„ä¿¡æ¯"""
        question = f"æˆ‘æ­£åœ¨çœ‹ä¸€ä¸ª {component_name}ï¼Œè¯·å‘Šè¯‰æˆ‘å®ƒåœ¨è¿™ä¸ªç”µè·¯ä¸­æ˜¯æ€ä¹ˆè¿æ¥çš„ï¼Ÿ"
        return self.ask(question, circuit_context)

    @staticmethod
    def _build_system_prompt(circuit_context: str, rag_context: str = "") -> str:
        # æ„å»ºçŸ¥è¯†åº“å‚è€ƒéƒ¨åˆ†
        knowledge_section = ""
        if rag_context:
            knowledge_section = f"""\n\nä½ è¿˜æ‹¥æœ‰ä»¥ä¸‹å‚è€ƒèµ„æ–™ (æ¥è‡ªæœ¬åœ°èŠ¯ç‰‡æ‰‹å†Œ/å®éªŒçŸ¥è¯†åº“):
{rag_context}
è¯·ä¼˜å…ˆå‚è€ƒè¿™äº›èµ„æ–™å›ç­”, å¹¶åœ¨å›ç­”ä¸­ç®€è¦æ³¨æ˜å‡ºå¤„ã€‚"""

        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå­å®éªŒå®¤åŠ©æ‰‹ (LabGuardian)ã€‚
ä½ æ‹¥æœ‰è®¡ç®—æœºè§†è§‰ç³»ç»Ÿæä¾›çš„å®æ—¶ç”µè·¯ç½‘è¡¨æ•°æ®ï¼š
{circuit_context if circuit_context else '(å½“å‰æš‚æ— ç”µè·¯æ•°æ®)'}
{knowledge_section}
è¯·åŸºäºæ­¤ç”µè·¯çŠ¶æ€å’Œå‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
- å¦‚æœè¢«é—®åŠè¿æ¥ï¼Œè¯·æ ¹æ® Net (ç½‘ç»œ) ä¿¡æ¯åˆ¤æ–­ã€‚
- Push_Button = æŒ‰é’®å¼€å…³, Wire = å¯¼çº¿ã€‚
- è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œåƒä¸€ä¸ªäººç±»åŠ©æ•™ä¸€æ ·è‡ªç„¶ã€‚
- å›ç­”ç®€æ´ï¼Œä¸è¦è¶…è¿‡ 300 å­—ã€‚"""
